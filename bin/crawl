#!/bin/bash

# Script to crawl a website and extract PDF information
# Usage: bin/crawl <site_url> <output_dir> [previous_crawl_directory]
# Site site_url must exist in python_components/crawler/config.json.

# Check if site_url parameter is provided
if [ $# -eq 0 ]; then
    echo "Error: site_url parameter is required"
    echo "Usage: bin/crawl <site_url> <output_dir> [previous_crawl_directory]"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09 /path/to/previous_crawl_directory/"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09 /path/to/previous_crawl_directory.zip"
    exit 1
fi

SITE_URL="$1"
OUTPUT_DIR="$2"
PREVIOUS_CRAWL="$3"

# Find the project root directory by looking for the bin directory
# This allows the script to work from any subdirectory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

docker build -t asap_pdf:crawler "$PROJECT_ROOT/python_components/crawler/."
docker build -t asap_pdf:classifier "$PROJECT_ROOT/python_components/classifier/."

CONFIG_FILE="$PROJECT_ROOT/python_components/crawler/config.json"

if [ ! -f "$CONFIG_FILE" ]; then
    echo "Error: config.json not found at $CONFIG_FILE"
    exit 1
fi

OUTPUT_FILE=$(jq -r --arg site_url "$SITE_URL" '
    if has($site_url) then
        .[$site_url].output_file
    else
        "output.csv"
    end
' "$CONFIG_FILE" 2>/dev/null || echo "output.csv")

if [ -n "$PREVIOUS_CRAWL" ]; then
    if [ ! -e "$PREVIOUS_CRAWL" ]; then
        echo "Error: Previous crawl path '$PREVIOUS_CRAWL' does not exist"
        exit 1
    fi
    PREVIOUS_CRAWL_DIR=$(mktemp -d)
    if [ -f "$PREVIOUS_CRAWL" ] && [[ "$PREVIOUS_CRAWL" == *.zip ]]; then
        if command -v unzip >/dev/null 2>&1; then
          unzip  -j -q "$PREVIOUS_CRAWL" -d "$PREVIOUS_CRAWL_DIR"
          if [ $? -eq 0 ]; then
              echo "Successfully extracted previous crawl archive"
          else
              echo "Error: Failed to extract zip archive"
              rm -rf "$PREVIOUS_CRAWL_DIR"
              exit 1
          fi
      else
          echo "Error: unzip command not found. Please install unzip to extract archives"
          rm -rf "$PREVIOUS_CRAWL_DIR"
          exit 1
      fi
    else
      cp -r "$PREVIOUS_CRAWL"/* "$PREVIOUS_CRAWL_DIR"
    fi
  COMPARISON_VOLUME="-v $PREVIOUS_CRAWL_DIR:/comparison"
  COMPARISON_FLAG="--comparison_crawl=/comparison/$OUTPUT_FILE"
fi

TMP_OUTPUT=$(mktemp -d)
mkdir -p $OUTPUT_DIR
echo "$OUTPUT_DIR"

set -x
if [ -n "$COMPARISON_VOLUME" ]; then
docker run --rm \
  -v "$PROJECT_ROOT/python_components/crawler:/workspace" \
  -v "$TMP_OUTPUT:/output" \
  $COMPARISON_VOLUME \
  asap_pdf:crawler \
  python /workspace/crawler.py "$SITE_URL" "/output/$OUTPUT_FILE" "$COMPARISON_FLAG"
else
  docker run --rm \
  -v "$PROJECT_ROOT/python_components/crawler:/workspace" \
  -v "$TMP_OUTPUT:/output" \
  asap_pdf:crawler \
  python /workspace/crawler.py "$SITE_URL" "/output/$OUTPUT_FILE"
fi

set +x

mv "$TMP_OUTPUT/$OUTPUT_FILE" "$TMP_OUTPUT/$OUTPUT_FILE-crawled"

set -x

docker run --rm \
  -v "$PROJECT_ROOT/python_components/classifier:/workspace" \
  -v "$TMP_OUTPUT:/output" \
  asap_pdf:classifier \
  python /workspace/classifier.py "/output/$OUTPUT_FILE-crawled" "/output/$OUTPUT_FILE"

set +x

mv "$TMP_OUTPUT/$OUTPUT_FILE" "$OUTPUT_DIR"

rm -rf "$TMP_OUTPUT"
if [ -n "$PREVIOUS_CRAWL" ]; then
  rm -rf "$PREVIOUS_CRAWL_DIR"
fi
