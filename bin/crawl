#!/bin/bash

# Script to crawl a website and extract PDF information
# Usage: bin/crawl <site_url> <output_dir> [previous_crawl_directory] [previous_link_json]
# Site site_url must exist in python_components/crawler/config.json.
# previous_crawl_directory is the path to a previous version of the crawl to compare against.
# previous_link_json is a list of links and sources produced by the first phase of the crawler.
#     Useful if metadata fetching failed or to resume an otherwise halted process.

# Check if site_url parameter is provided
if [ $# -eq 0 ]; then
    echo "Error: site_url parameter is required"
    echo "Usage: bin/crawl <site_url> <output_dir> [previous_crawl_directory] [previous_link_json]"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09 /path/to/previous_crawl_directory/"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09 /path/to/previous_crawl_directory.zip"
    echo "Example: bin/crawl https://georgia.gov /db/seeds/site_documents_2025_09 /path/to/previous_crawl_directory.zip /georgia_files.json"
    exit 1
fi

SITE_URL="$1"
OUTPUT_DIR="$2"
PREVIOUS_CRAWL="$3"
PREVIOUS_LINK_JSON="$4"

# Find the project root directory by looking for the bin directory
# This allows the script to work from any subdirectory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

docker build -t asap_pdf:crawler "$PROJECT_ROOT/python_components/crawler/."
docker build -t asap_pdf:classifier "$PROJECT_ROOT/python_components/classifier/."

CONFIG_FILE="$PROJECT_ROOT/python_components/crawler/config.json"

if [ ! -f "$CONFIG_FILE" ]; then
    echo "Error: config.json not found at $CONFIG_FILE"
    exit 1
fi

OUTPUT_FILE=$(jq -r --arg site_url "$SITE_URL" '
    if has($site_url) then
        .[$site_url].output_file
    else
        "output.csv"
    end
' "$CONFIG_FILE" 2>/dev/null || echo "output.csv")

if [ -n "$PREVIOUS_CRAWL" ]; then
    if [ ! -e "$PREVIOUS_CRAWL" ]; then
        echo "Error: Previous crawl path '$PREVIOUS_CRAWL' does not exist"
        exit 1
    fi
    CRAWLER_TMP_DIR="$PROJECT_ROOT/crawler_tmp"
    PREVIOUS_CRAWL_DIR="$CRAWLER_TMP_DIR/previous_crawl"
    mkdir -p $PREVIOUS_CRAWL_DIR
    if [ -f "$PREVIOUS_CRAWL" ] && [[ "$PREVIOUS_CRAWL" == *.zip ]]; then
        if command -v unzip >/dev/null 2>&1; then
          unzip  -j -q "$PREVIOUS_CRAWL" -d "$PREVIOUS_CRAWL_DIR"
          if [ $? -eq 0 ]; then
              echo "Successfully extracted previous crawl archive"
          else
              echo "Error: Failed to extract zip archive"
              rm -rf "$PREVIOUS_CRAWL_DIR"
              exit 1
          fi
      else
          echo "Error: unzip command not found. Please install unzip to extract archives"
          rm -rf "$PREVIOUS_CRAWL_DIR"
          exit 1
      fi
    else
      cp -r "$PREVIOUS_CRAWL"/* "$PREVIOUS_CRAWL_DIR"
    fi
  COMPARISON_FLAG="--comparison_crawl=/data/previous_crawl/$OUTPUT_FILE"
fi

PREVIOUS_JSON_LINK_FLAG=""
if [ -n "$PREVIOUS_LINK_JSON" ]; then
  if [ ! -f "$PREVIOUS_LINK_JSON"  ]; then
    echo "Previous JSON crawl file was specified, but does not exist."
    exit 1
  fi
  mkdir -p "$CRAWLER_TMP_DIR/previous_json_links"
  mv $PREVIOUS_LINK_JSON "$CRAWLER_TMP_DIR/previous_json_links"
  PREVIOUS_JSON_LINK_FLAG="--crawled_links_json=/data/previous_json_links/$(basename $PREVIOUS_LINK_JSON)"
fi

TMP_OUTPUT="$CRAWLER_TMP_DIR/output"
mkdir -p $TMP_OUTPUT
mkdir -p $OUTPUT_DIR
echo "$OUTPUT_DIR"

set -x

crawler_command=(
    docker run --rm
    -v "$PROJECT_ROOT/python_components/crawler:/workspace"
    -v "$CRAWLER_TMP_DIR:/data"
    asap_pdf:crawler
    python /workspace/crawler.py "$SITE_URL" "/data/output/$OUTPUT_FILE"
)

if [ -n "$COMPARISON_FLAG" ]; then
  crawler_command+=("$COMPARISON_FLAG")
fi

if [ -n "$PREVIOUS_JSON_LINK_FLAG" ]; then
     crawler_command+=("$PREVIOUS_JSON_LINK_FLAG")
fi

"${crawler_command[@]}"

set +x

mv "$TMP_OUTPUT/$OUTPUT_FILE" "$TMP_OUTPUT/$OUTPUT_FILE-crawled"

set -x

docker run --rm \
  -v "$PROJECT_ROOT/python_components/classifier:/workspace" \
  -v "$TMP_OUTPUT:/output" \
  asap_pdf:classifier \
  python /workspace/classifier.py "/output/$OUTPUT_FILE-crawled" "/output/$OUTPUT_FILE"

set +x

mv "$TMP_OUTPUT/$OUTPUT_FILE" "$OUTPUT_DIR"

rm -rf "$CRAWLER_TMP_DIR"
